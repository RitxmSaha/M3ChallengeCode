{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class WFHEmployeeDataset(Dataset):\n",
    "  \"\"\"WFH Survey Dataset, targeting employee wishes for WFH.\"\"\"\n",
    "\n",
    "  def __init__(self, csv_file, transform=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        csv_file (string): Path to the csv file with annotations.\n",
    "        root_dir (string): Directory with all the images.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.variables = [\n",
    "      \"income\",\n",
    "      \"age_quant\",\n",
    "      \"educ_years\",\n",
    "      \"wfhcovid_frac\",\n",
    "      \"commutetime_quant\",\n",
    "      \"wfh_eff_COVID_quant\",\n",
    "      \"gender\",\n",
    "      \"wfh_days_postCOVID_ss\",\n",
    "      \"live_children\",\n",
    "      \"race_ethnicity\",\n",
    "      \"hourly_wage\",\n",
    "      \"workhours_preCOVID\",\n",
    "      \"workhours_duringCOVID\",\n",
    "      \"downloadspeed\",\n",
    "      \"uploadspeed\",\n",
    "      \"grass_color_attnfull\",\n",
    "      \"party_affiliation_s\",\n",
    "    ]\n",
    "\n",
    "    self.data = pd.read_csv(csv_file, usecols=self.variables)\n",
    "\n",
    "    # Data Filtering\n",
    "    # Must have answered 3 for the questionnaire to be filled out correctly\n",
    "    self.data = self.data[self.data.grass_color_attnfull == 3]\n",
    "    self.data = self.data.fillna(value={\"workhours_preCOVID\": 0, \"workhours_duringCOVID\": 0, \"party_affiliation_s\":0})\n",
    "    self.data = self.data.dropna()\n",
    "\n",
    "    print(len(data))\n",
    "\n",
    "    self.transform = transform\n",
    "\n",
    "    self.targets = np.array(self.data['wfh_days_postCOVID_ss'] == 1).astype(np.float32)\n",
    "\n",
    "    self.variables.remove(\"wfh_days_postCOVID_ss\")\n",
    "\n",
    "    self.features = self.data[self.variables].to_numpy(dtype='float32')\n",
    "\n",
    "    # .astype(np.float32) is necessary because PyTorch doesn't like float64\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    target = self.targets[idx]\n",
    "    features = self.features[idx]\n",
    "\n",
    "    if self.transform:\n",
    "      features = self.transform(features)\n",
    "\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5151\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data = WFHEmployeeDataset('WFHdata_January22.csv', transform=torch.tensor)\n",
    "\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(\n",
    "    data, \n",
    "    [train_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 128)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer):\n",
    "  total_loss = 0.\n",
    "  iter = 0\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for data, target in loader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    data = data\n",
    "    pred = model(data)\n",
    "    loss = loss_function(pred.view(-1), target.view(-1))\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    iter += 1\n",
    "  \n",
    "  return total_loss / iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "  total_loss = 0.\n",
    "  iter = 0\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data, target in loader:\n",
    "      data = data\n",
    "      pred = model(data)\n",
    "      loss = loss_function(pred.view(-1), target.view(-1))\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      iter += 1\n",
    "  \n",
    "  return total_loss / iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MLPModel, self).__init__()\n",
    "\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(16, 64),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.Dropout(p=0.5),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(32, 16),\n",
    "      nn.Dropout(p=0.5),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(16, 8),\n",
    "      nn.Dropout(p=0.5),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(8, 1)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPModel()\n",
    "\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.185813924128359 | Test Loss: 0.13339596500413287\n",
      "Epoch: 1 | Loss: 0.15714176292672302 | Test Loss: 0.10384277740700378\n",
      "Epoch: 2 | Loss: 0.14143514836376364 | Test Loss: 0.0960672048644887\n",
      "Epoch: 3 | Loss: 0.13657742377483484 | Test Loss: 0.09523353001309766\n",
      "Epoch: 4 | Loss: 0.1327425537235809 | Test Loss: 0.09568920565976037\n",
      "Epoch: 5 | Loss: 0.13230649223833374 | Test Loss: 0.09596805481447114\n",
      "Epoch: 6 | Loss: 0.1283673524405017 | Test Loss: 0.09567859727475378\n",
      "Epoch: 7 | Loss: 0.12358564160989993 | Test Loss: 0.09537872299551964\n",
      "Epoch: 8 | Loss: 0.12515240275498593 | Test Loss: 0.09502491561902894\n",
      "Epoch: 9 | Loss: 0.1223241960008939 | Test Loss: 0.09492791278494729\n",
      "Epoch: 10 | Loss: 0.12344582582061941 | Test Loss: 0.09441603637403911\n",
      "Epoch: 11 | Loss: 0.12162074649875815 | Test Loss: 0.09397064770261447\n",
      "Epoch: 12 | Loss: 0.1201124545751196 | Test Loss: 0.09363315420018302\n",
      "Epoch: 13 | Loss: 0.12276667672576326 | Test Loss: 0.09279983614881833\n",
      "Epoch: 14 | Loss: 0.11608570701245105 | Test Loss: 0.09224208485749033\n",
      "Epoch: 15 | Loss: 0.11766184708385756 | Test Loss: 0.09185052870048417\n",
      "Epoch: 16 | Loss: 0.1193164943745642 | Test Loss: 0.0914319795038965\n",
      "Epoch: 17 | Loss: 0.11636634679003195 | Test Loss: 0.09122221668561299\n",
      "Epoch: 18 | Loss: 0.11647140686259125 | Test Loss: 0.09070455655455589\n",
      "Epoch: 19 | Loss: 0.11350844823049777 | Test Loss: 0.09021315102775891\n",
      "Epoch: 20 | Loss: 0.11555258613644225 | Test Loss: 0.09009364392194483\n",
      "Epoch: 21 | Loss: 0.11503813189990593 | Test Loss: 0.08959500429530938\n",
      "Epoch: 22 | Loss: 0.11379068367408984 | Test Loss: 0.08926629419955942\n",
      "Epoch: 23 | Loss: 0.11350592519297745 | Test Loss: 0.08909080736339092\n",
      "Epoch: 24 | Loss: 0.11214210025288841 | Test Loss: 0.08905360268221961\n",
      "Epoch: 25 | Loss: 0.11021769949884126 | Test Loss: 0.08900412047902743\n",
      "Epoch: 26 | Loss: 0.11286943755818135 | Test Loss: 0.08886676592131455\n",
      "Epoch: 27 | Loss: 0.10953856349894495 | Test Loss: 0.08873493162294228\n",
      "Epoch: 28 | Loss: 0.1114223554278865 | Test Loss: 0.08827152227361997\n",
      "Epoch: 29 | Loss: 0.11120584313616608 | Test Loss: 0.08808641259868939\n",
      "Epoch: 30 | Loss: 0.10852327423565315 | Test Loss: 0.08775224391784933\n",
      "Epoch: 31 | Loss: 0.10685567490079186 | Test Loss: 0.08744209632277489\n",
      "Epoch: 32 | Loss: 0.10931076211008159 | Test Loss: 0.08736810067461596\n",
      "Epoch: 33 | Loss: 0.1091171745775324 | Test Loss: 0.08701580555902587\n",
      "Epoch: 34 | Loss: 0.10891568931666287 | Test Loss: 0.08657181242273913\n",
      "Epoch: 35 | Loss: 0.10884422108982549 | Test Loss: 0.08648765397568543\n",
      "Epoch: 36 | Loss: 0.10832969663721143 | Test Loss: 0.08647396311991745\n",
      "Epoch: 37 | Loss: 0.10902117712028099 | Test Loss: 0.0861190240830183\n",
      "Epoch: 38 | Loss: 0.10692744566635652 | Test Loss: 0.08610110212531355\n",
      "Epoch: 39 | Loss: 0.10705727564566063 | Test Loss: 0.08583428007033136\n",
      "Epoch: 40 | Loss: 0.10538584874434904 | Test Loss: 0.08577300359805425\n",
      "Epoch: 41 | Loss: 0.10601686234727051 | Test Loss: 0.0855043629805247\n",
      "Epoch: 42 | Loss: 0.10454462513779149 | Test Loss: 0.08561501776178677\n",
      "Epoch: 43 | Loss: 0.1053570480735013 | Test Loss: 0.08543327326575915\n",
      "Epoch: 44 | Loss: 0.10359162363139066 | Test Loss: 0.08558178568879764\n",
      "Epoch: 45 | Loss: 0.10479632087729195 | Test Loss: 0.0853705449650685\n",
      "Epoch: 46 | Loss: 0.1054336941151908 | Test Loss: 0.08529856014582846\n",
      "Epoch: 47 | Loss: 0.10341465890859113 | Test Loss: 0.08497095728913943\n",
      "Epoch: 48 | Loss: 0.10422969654654012 | Test Loss: 0.08488960564136505\n",
      "Epoch: 49 | Loss: 0.10390014540065419 | Test Loss: 0.08465536725189951\n",
      "Epoch: 50 | Loss: 0.10499797863039104 | Test Loss: 0.0845182558728589\n",
      "Epoch: 51 | Loss: 0.10354457186027007 | Test Loss: 0.08447069219417042\n",
      "Epoch: 52 | Loss: 0.10177768202442111 | Test Loss: 0.08446394114030732\n",
      "Epoch: 53 | Loss: 0.10339726078690904 | Test Loss: 0.08437711166010962\n",
      "Epoch: 54 | Loss: 0.10376549004153772 | Test Loss: 0.08429746060735649\n",
      "Epoch: 55 | Loss: 0.10183080116456206 | Test Loss: 0.08424556979702578\n",
      "Epoch: 56 | Loss: 0.10261887786063281 | Test Loss: 0.08430520320932071\n",
      "Epoch: 57 | Loss: 0.10324860680283922 | Test Loss: 0.08423104447623093\n",
      "Epoch: 58 | Loss: 0.10178371880090598 | Test Loss: 0.08411045662230915\n",
      "Epoch: 59 | Loss: 0.10247784028902199 | Test Loss: 0.08403880335390568\n",
      "Epoch: 60 | Loss: 0.10316122379718405 | Test Loss: 0.08368519900573625\n",
      "Epoch: 61 | Loss: 0.10094963285056027 | Test Loss: 0.08355178352859285\n",
      "Epoch: 62 | Loss: 0.10073006875587232 | Test Loss: 0.08361426099307007\n",
      "Epoch: 63 | Loss: 0.10084759839104884 | Test Loss: 0.08351769070658419\n",
      "Epoch: 64 | Loss: 0.1020000817423517 | Test Loss: 0.08328134794202116\n",
      "Epoch: 65 | Loss: 0.10081650802131856 | Test Loss: 0.08335045083529419\n",
      "Epoch: 66 | Loss: 0.09986940071438298 | Test Loss: 0.0832549733006292\n",
      "Epoch: 67 | Loss: 0.10050715393189227 | Test Loss: 0.08317648350364631\n",
      "Epoch: 68 | Loss: 0.10028029571879994 | Test Loss: 0.0833767412437333\n",
      "Epoch: 69 | Loss: 0.10126529148582256 | Test Loss: 0.08319506607949734\n",
      "Epoch: 70 | Loss: 0.10043527163339383 | Test Loss: 0.08311736376749145\n",
      "Epoch: 71 | Loss: 0.09965306384996934 | Test Loss: 0.08299652797480424\n",
      "Epoch: 72 | Loss: 0.09937791436007529 | Test Loss: 0.08300600138803323\n",
      "Epoch: 73 | Loss: 0.10053211698929469 | Test Loss: 0.08309316821396351\n",
      "Epoch: 74 | Loss: 0.09952028479539987 | Test Loss: 0.08291569621198708\n",
      "Epoch: 75 | Loss: 0.10090141314448732 | Test Loss: 0.08293667311469714\n",
      "Epoch: 76 | Loss: 0.09885107059821938 | Test Loss: 0.08261777129438189\n",
      "Epoch: 77 | Loss: 0.09988444558147228 | Test Loss: 0.08272216696706083\n",
      "Epoch: 78 | Loss: 0.09892062007477789 | Test Loss: 0.08256051337553395\n",
      "Epoch: 79 | Loss: 0.09936668622222813 | Test Loss: 0.08256835945778424\n",
      "Epoch: 80 | Loss: 0.10033832524310458 | Test Loss: 0.08251283048755592\n",
      "Epoch: 81 | Loss: 0.09999799141378114 | Test Loss: 0.08242182888918453\n",
      "Epoch: 82 | Loss: 0.09881773681351633 | Test Loss: 0.08257172194619973\n",
      "Epoch: 83 | Loss: 0.09833486626545589 | Test Loss: 0.0823687178393205\n",
      "Epoch: 84 | Loss: 0.09990810173930544 | Test Loss: 0.08223935320145553\n",
      "Epoch: 85 | Loss: 0.09858095973278537 | Test Loss: 0.08227664832439688\n",
      "Epoch: 86 | Loss: 0.09853075343099507 | Test Loss: 0.08217135878900687\n",
      "Epoch: 87 | Loss: 0.0985394875434312 | Test Loss: 0.08205089418010579\n",
      "Epoch: 88 | Loss: 0.09750398869315784 | Test Loss: 0.08227433409127924\n",
      "Epoch: 89 | Loss: 0.09828763489018787 | Test Loss: 0.08215468873580296\n",
      "Epoch: 90 | Loss: 0.09952904532353084 | Test Loss: 0.08218982008596261\n",
      "Epoch: 91 | Loss: 0.09775421136256421 | Test Loss: 0.08217119011614057\n",
      "Epoch: 92 | Loss: 0.09842820115613216 | Test Loss: 0.08216833675073253\n",
      "Epoch: 93 | Loss: 0.09792338672912482 | Test Loss: 0.08221314433548185\n",
      "Epoch: 94 | Loss: 0.09709569168361751 | Test Loss: 0.0821895305481222\n",
      "Epoch: 95 | Loss: 0.09873932701620189 | Test Loss: 0.08217545060647859\n",
      "Epoch: 96 | Loss: 0.09837063006830937 | Test Loss: 0.08209928083750936\n",
      "Epoch: 97 | Loss: 0.09762779126564662 | Test Loss: 0.08194149409731229\n",
      "Epoch: 98 | Loss: 0.09698848697272214 | Test Loss: 0.08194795157760382\n",
      "Epoch: 99 | Loss: 0.09680365533991293 | Test Loss: 0.08185227525730927\n",
      "Epoch: 100 | Loss: 0.09731336750767448 | Test Loss: 0.08184802977161275\n",
      "Epoch: 101 | Loss: 0.09738540784879164 | Test Loss: 0.0818163917089502\n",
      "Epoch: 102 | Loss: 0.09764391274163217 | Test Loss: 0.0818624770682719\n",
      "Epoch: 103 | Loss: 0.09737539426846938 | Test Loss: 0.08183578567372428\n",
      "Epoch: 104 | Loss: 0.09651047273567229 | Test Loss: 0.08199380441672272\n",
      "Epoch: 105 | Loss: 0.09663944939772288 | Test Loss: 0.08198999075425996\n",
      "Epoch: 106 | Loss: 0.09655067144018231 | Test Loss: 0.08199411961767408\n",
      "Epoch: 107 | Loss: 0.09672518605084131 | Test Loss: 0.08187735774036911\n",
      "Epoch: 108 | Loss: 0.0973129930595557 | Test Loss: 0.08190608055641253\n",
      "Epoch: 109 | Loss: 0.09674943046587886 | Test Loss: 0.08198413480487135\n",
      "Epoch: 110 | Loss: 0.09461158220515106 | Test Loss: 0.08190540079441336\n",
      "Epoch: 111 | Loss: 0.09696601376389012 | Test Loss: 0.08180368350197871\n",
      "Epoch: 112 | Loss: 0.09570577718091733 | Test Loss: 0.08179871023943026\n",
      "Epoch: 113 | Loss: 0.09677996418692848 | Test Loss: 0.08183072497033411\n",
      "Epoch: 114 | Loss: 0.09675048054619269 | Test Loss: 0.08170036495559746\n",
      "Epoch: 115 | Loss: 0.09600541932564793 | Test Loss: 0.08179707897620069\n",
      "Epoch: 116 | Loss: 0.09595306172515407 | Test Loss: 0.08183032191461986\n",
      "Epoch: 117 | Loss: 0.09495592772057562 | Test Loss: 0.08179995376202795\n",
      "Epoch: 118 | Loss: 0.09607384850581487 | Test Loss: 0.0818565594446328\n",
      "Epoch: 119 | Loss: 0.09640987702842915 | Test Loss: 0.08206868709789382\n",
      "Epoch: 120 | Loss: 0.09708025678992271 | Test Loss: 0.08169007725599739\n",
      "Epoch: 121 | Loss: 0.09560531174594705 | Test Loss: 0.08163004171931082\n",
      "Epoch: 122 | Loss: 0.09519179323405931 | Test Loss: 0.08165068810598718\n",
      "Epoch: 123 | Loss: 0.09735179376421553 | Test Loss: 0.0817375508033567\n",
      "Epoch: 124 | Loss: 0.0964563029507796 | Test Loss: 0.08176141946266095\n",
      "Epoch: 125 | Loss: 0.09552896282438075 | Test Loss: 0.08184013153529829\n",
      "Epoch: 126 | Loss: 0.0964470494425658 | Test Loss: 0.08177290349784824\n",
      "Epoch: 127 | Loss: 0.09528179240949226 | Test Loss: 0.08170582550681299\n",
      "Epoch: 128 | Loss: 0.09580566858251889 | Test Loss: 0.08182663636075126\n",
      "Epoch: 129 | Loss: 0.09463309970769015 | Test Loss: 0.08179793806953563\n",
      "Epoch: 130 | Loss: 0.09408911938468616 | Test Loss: 0.08179319505062368\n",
      "Epoch: 131 | Loss: 0.09604276197426247 | Test Loss: 0.081895863947769\n",
      "Epoch: 132 | Loss: 0.09505474488392021 | Test Loss: 0.08204300484309594\n",
      "Epoch: 133 | Loss: 0.09509668047681 | Test Loss: 0.08201292115781042\n",
      "Epoch: 134 | Loss: 0.0963387657521349 | Test Loss: 0.08207959598965114\n",
      "Epoch: 135 | Loss: 0.09398141774264249 | Test Loss: 0.0820382713443703\n",
      "Epoch: 136 | Loss: 0.09466965654582689 | Test Loss: 0.08198370339555873\n",
      "Epoch: 137 | Loss: 0.09484041267723749 | Test Loss: 0.08186848668588532\n",
      "Epoch: 138 | Loss: 0.0959499811358524 | Test Loss: 0.08197127800020906\n",
      "Epoch: 139 | Loss: 0.09388915754177353 | Test Loss: 0.08215749284459485\n",
      "Epoch: 140 | Loss: 0.09378163132703665 | Test Loss: 0.08206557244476345\n",
      "Epoch: 141 | Loss: 0.0947014552851518 | Test Loss: 0.08203524268335766\n",
      "Epoch: 142 | Loss: 0.09604966087323247 | Test Loss: 0.08205122852491008\n",
      "Epoch: 143 | Loss: 0.09579672280586127 | Test Loss: 0.08210997221370538\n",
      "Epoch: 144 | Loss: 0.09416040179855896 | Test Loss: 0.08200103975832462\n",
      "Epoch: 145 | Loss: 0.09410968151959506 | Test Loss: 0.08209843985322449\n",
      "Epoch: 146 | Loss: 0.09429604661735622 | Test Loss: 0.08210521336230966\n",
      "Epoch: 147 | Loss: 0.09431083839048039 | Test Loss: 0.08208899696667989\n",
      "Epoch: 148 | Loss: 0.09410689195448702 | Test Loss: 0.08207139279693365\n",
      "Epoch: 149 | Loss: 0.09419746087356047 | Test Loss: 0.08197685651895073\n",
      "Epoch: 150 | Loss: 0.09432186321778731 | Test Loss: 0.08203648775815964\n",
      "Epoch: 151 | Loss: 0.09439568413477956 | Test Loss: 0.08213660524537165\n",
      "Epoch: 152 | Loss: 0.09431510192878319 | Test Loss: 0.08204834173536962\n",
      "Epoch: 153 | Loss: 0.09336555557269038 | Test Loss: 0.08219261767549647\n",
      "Epoch: 154 | Loss: 0.09376742837555481 | Test Loss: 0.0820516346850329\n",
      "Epoch: 155 | Loss: 0.09377110772060626 | Test Loss: 0.08213660390012795\n",
      "Epoch: 156 | Loss: 0.09374886186737003 | Test Loss: 0.08235165870024098\n",
      "Epoch: 157 | Loss: 0.09516296659906705 | Test Loss: 0.08225085410392946\n",
      "Epoch: 158 | Loss: 0.09374580803242596 | Test Loss: 0.08251545278148519\n",
      "Epoch: 159 | Loss: 0.09248529814861038 | Test Loss: 0.08235359709295961\n",
      "Epoch: 160 | Loss: 0.09314398738470944 | Test Loss: 0.08234065191613303\n",
      "Epoch: 161 | Loss: 0.09410639649087732 | Test Loss: 0.08235050913774306\n",
      "Epoch: 162 | Loss: 0.09288206617489006 | Test Loss: 0.0823655424432622\n",
      "Epoch: 163 | Loss: 0.09384249139464262 | Test Loss: 0.08237272521687879\n",
      "Epoch: 164 | Loss: 0.09276806201898691 | Test Loss: 0.08250479896863301\n",
      "Epoch: 165 | Loss: 0.09283719279549339 | Test Loss: 0.08244776436024243\n",
      "Epoch: 166 | Loss: 0.09315115563345677 | Test Loss: 0.08235179726034403\n",
      "Epoch: 167 | Loss: 0.09350042270891594 | Test Loss: 0.08265940969189008\n",
      "Epoch: 168 | Loss: 0.09278541634028609 | Test Loss: 0.08272251838611232\n",
      "Epoch: 169 | Loss: 0.09257839371760686 | Test Loss: 0.08280224249594742\n",
      "Epoch: 170 | Loss: 0.09283472010583589 | Test Loss: 0.08263692114916113\n",
      "Epoch: 171 | Loss: 0.09342504812009407 | Test Loss: 0.082747555958728\n",
      "Epoch: 172 | Loss: 0.09309238748568477 | Test Loss: 0.08257366292592552\n",
      "Epoch: 173 | Loss: 0.09295281609802535 | Test Loss: 0.08261521967748801\n",
      "Epoch: 174 | Loss: 0.09240127095218861 | Test Loss: 0.08270533972730239\n",
      "Epoch: 175 | Loss: 0.09253924543207342 | Test Loss: 0.08278334813399447\n",
      "Epoch: 176 | Loss: 0.09215846962549469 | Test Loss: 0.08288275740212864\n",
      "Epoch: 177 | Loss: 0.09320578520948236 | Test Loss: 0.08289090916514397\n",
      "Epoch: 178 | Loss: 0.09199301593682983 | Test Loss: 0.08281243489020401\n",
      "Epoch: 179 | Loss: 0.0917659964073788 | Test Loss: 0.08276298569722308\n",
      "Epoch: 180 | Loss: 0.09181048833962643 | Test Loss: 0.08296352335148388\n",
      "Epoch: 181 | Loss: 0.09219185906377705 | Test Loss: 0.08292033750977781\n",
      "Epoch: 182 | Loss: 0.09165283315109485 | Test Loss: 0.08288142861177523\n",
      "Epoch: 183 | Loss: 0.0911523172575416 | Test Loss: 0.08318701676196522\n",
      "Epoch: 184 | Loss: 0.0909055902650862 | Test Loss: 0.08327139934731854\n",
      "Epoch: 185 | Loss: 0.09256165813315999 | Test Loss: 0.08318756044738823\n",
      "Epoch: 186 | Loss: 0.09356740026762991 | Test Loss: 0.08303938712924719\n",
      "Epoch: 187 | Loss: 0.09216695448214357 | Test Loss: 0.08307390163342158\n",
      "Epoch: 188 | Loss: 0.09260160532413107 | Test Loss: 0.08331367021633519\n",
      "Epoch: 189 | Loss: 0.09124928159695683 | Test Loss: 0.08337280257708496\n",
      "Epoch: 190 | Loss: 0.09352806120207816 | Test Loss: 0.08323239183260335\n",
      "Epoch: 191 | Loss: 0.09200766795512402 | Test Loss: 0.0831114790505833\n",
      "Epoch: 192 | Loss: 0.09169605340469968 | Test Loss: 0.08316715496281783\n",
      "Epoch: 193 | Loss: 0.09123853605353471 | Test Loss: 0.08316954742703173\n",
      "Epoch: 194 | Loss: 0.09129786604281628 | Test Loss: 0.08311081615587075\n",
      "Epoch: 195 | Loss: 0.09164246555530664 | Test Loss: 0.08315973273581928\n",
      "Epoch: 196 | Loss: 0.09195517545396631 | Test Loss: 0.0830920801187555\n",
      "Epoch: 197 | Loss: 0.09082053816228201 | Test Loss: 0.08334364607516262\n",
      "Epoch: 198 | Loss: 0.09178834950382059 | Test Loss: 0.08324960846867827\n",
      "Epoch: 199 | Loss: 0.09112949612917322 | Test Loss: 0.08326834295358923\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "  loss = train(mlp_model, train_loader, optimizer)\n",
    "  test_loss = test(mlp_model, test_loader)\n",
    "  print(f'Epoch: {epoch} | Loss: {loss:4} | Test Loss: {test_loss:4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5151\n"
     ]
    }
   ],
   "source": [
    "data = WFHEmployeeDataset('WFHdata_January22.csv', transform=torch.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.9696904021095\n",
      "8.957967992092463\n",
      "2.1863018830184626\n",
      "37.24955670274001\n",
      "25.24253979467531\n",
      "17.41056695365713\n",
      "0.4868630040849206\n",
      "1.1249208286974446\n",
      "1.9440096215793632\n",
      "641.9956642495715\n",
      "15.201763562440355\n",
      "15.32297187230933\n",
      "128.93218933500688\n",
      "107.70580571693812\n",
      "0.0\n",
      "0.9226649295101861\n",
      "{'income': 0.32286134, 'age_quant': 0.043398183, 'educ_years': 0.009114683, 'wfhcovid_frac': 0.12647003, 'commutetime_quant': 0.08064015, 'wfh_eff_COVID_quant': 0.109367594, 'gender': 0.0006728243, 'live_children': 0.0018368693, 'race_ethnicity': 0.0067131645, 'hourly_wage': 0.22932464, 'workhours_preCOVID': 0.052451037, 'workhours_duringCOVID': 0.05553634, 'downloadspeed': 0.16223021, 'uploadspeed': 0.17457998, 'grass_color_attnfull': 0.0, 'party_affiliation_s': 0.0011143773}\n"
     ]
    }
   ],
   "source": [
    "mlp_model.eval()\n",
    "\n",
    "columns = data.variables\n",
    "\n",
    "bvs = []\n",
    "features = data.data[data.variables].to_numpy(dtype='float32')\n",
    "for survey in features:\n",
    "    bvs.append(mlp_model(torch.tensor(survey)).detach().numpy()[0])\n",
    "bvs = np.array(bvs)\n",
    "\n",
    "mean_dev = {}\n",
    "\n",
    "for variable in columns:\n",
    "    std_dev = np.std(data.data[variable])\n",
    "    print(std_dev)\n",
    "\n",
    "    difference = 0\n",
    "    \n",
    "    # Subtract 1 std. dev\n",
    "    data.data[variable] = data.data[variable] - std_dev\n",
    "    features = data.data[data.variables].to_numpy(dtype='float32')\n",
    "\n",
    "    adj = []\n",
    "    for survey in features:\n",
    "        adj.append(mlp_model(torch.tensor(survey)).detach().numpy()[0])\n",
    "    adj = np.array(adj)\n",
    "\n",
    "    diff = np.abs(adj - bvs)\n",
    "\n",
    "    # Add 1 std. dev (add two due to subtracting one earlier)\n",
    "    data.data[variable] = data.data[variable] + 2 * std_dev\n",
    "    features = data.data[data.variables].to_numpy(dtype='float32')\n",
    "\n",
    "    adj = []\n",
    "    for survey in features:\n",
    "        adj.append(mlp_model(torch.tensor(survey)).detach().numpy()[0])\n",
    "    adj = np.array(adj)\n",
    "\n",
    "    diff = diff + np.abs(adj - bvs)\n",
    "\n",
    "    # Reset by subtracting 1 std. dev again\n",
    "    data.data[variable] = data.data[variable] - std_dev\n",
    "\n",
    "    mean_dev[variable] = diff.mean()\n",
    "\n",
    "print(mean_dev)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
